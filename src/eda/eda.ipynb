{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e994125",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Análisis exploratorio de datos (EDA)\"\n",
    "subtitle: \"Home Credit Default Risk\"\n",
    "author:\n",
    "  - name: \"Gerardo Guerrero\"\n",
    "  - name: \"Juan Pablo Cordero\"\n",
    "  - name: \"Jerónimo Deli\"\n",
    "  - name: \"Romain S\"\n",
    "date: \"2025-12-09\"\n",
    "format:\n",
    "  html:\n",
    "    theme: default\n",
    "    toc: false\n",
    "    code-fold: true\n",
    "    number-sections: true\n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "  error: false\n",
    "jupyter: python3\n",
    "editor: visual\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from IPython.display import Markdown\n",
    "\n",
    "#display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3647c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "applications = pd.read_csv('../../data/home-credit-default-risk/application_train.csv')\n",
    "applications.columns = applications.columns.str.lower()\n",
    "applications.Name = 'applications'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5fb58",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este cuaderno, realizaremos un análisis exploratorio de datos (EDA) en el conjunto de datos \"Home Credit Default Risk\" de [Home Credit Group](https://www.kaggle.com/competitions/home-credit-default-risk/data). El objetivo es comprender mejor las características de los datos previo al desarrollo de modelos predictivos.\n",
    "\n",
    "![dataset-description](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58b8a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>proportion</th>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.080729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1\n",
       "proportion  0.919271  0.080729"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(applications.target.value_counts(normalize=True).to_frame().reset_index(drop=True).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46dfb75",
   "metadata": {},
   "source": [
    "Estamos ante un caso de clasificación binaria, donde el objetivo es predecir si un cliente incumplirá con el pago de un préstamo. La variable objetivo es `TARGET`, que toma el valor 1 si el cliente incumple y 0 en caso contrario.\n",
    "\n",
    "Esto representa un caso de desbalanceo de clases, ya que la mayoría de los clientes no incumplen con sus pagos. Sin embargo, es crucial identificar a aquellos que sí lo harán para mitigar riesgos financieros. \n",
    "\n",
    "En ese sentido, buscaremos obtener un modelo que optimice PR-AUC (Precision-Recall Area Under Curve), ya que nos interesa maximizar el recall minimizando los falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1063b",
   "metadata": {},
   "source": [
    "# Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9f1d8",
   "metadata": {},
   "source": [
    "Para evitar el data snooping bias, realizamos la división del conjunto de datos en train y test antes de realizar el análisis exploratorio de datos.\n",
    "\n",
    "Además es importante que esta división se realice de manera estratificada por la variable objetivo, para asegurar que ambas particiones mantengan la misma proporción de clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218b51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = applications.drop(columns=['target'])  # Variables predictoras\n",
    "y = applications['target']                 # Variable objetivo\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "app = pd.concat([X_train, y_train], axis=1)\n",
    "app.Name = 'applications_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c12753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_between_features(df, features, top_k=20):\n",
    "    corr_matrix = df[features].corr().abs()\n",
    "    \n",
    "    # Máscara para quedarte solo con la mitad superior\n",
    "    mask = np.tril(np.ones_like(corr_matrix, dtype=bool), k=0)\n",
    "    corr_tri = corr_matrix.mask(mask)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(corr_tri, cmap='coolwarm', vmin=0, vmax=1)\n",
    "    plt.colorbar(im, label='Correlación absoluta')\n",
    "\n",
    "    plt.xticks(ticks=np.arange(len(features)), labels=features, rotation=90)\n",
    "    plt.yticks(ticks=np.arange(len(features)), labels=features)\n",
    "\n",
    "    plt.title('Matriz de correlación')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Obtener solo la mitad superior para evitar duplicados y diagonal\n",
    "    pairs = []\n",
    "    n = len(features)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            pairs.append((\n",
    "                features[i],\n",
    "                features[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "    # Ordenar por correlación de mayor a menor\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[2], reverse=True)[:top_k]\n",
    "\n",
    "    # Preparar datos para graficar\n",
    "    pair_labels = [f\"{a} – {b}\" for a, b, _ in pairs_sorted]\n",
    "    corr_values = [c for _, _, c in pairs_sorted]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 0.4 * top_k + 1))\n",
    "\n",
    "    # “Heatmap” vertical de una sola columna\n",
    "    im = ax.imshow(\n",
    "        np.array(corr_values)[..., None],  # shape (top_k, 1)\n",
    "        cmap='coolwarm',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        aspect='auto'\n",
    "    )\n",
    "\n",
    "    # Etiquetas en el eje y: los pares\n",
    "    ax.set_yticks(np.arange(top_k))\n",
    "    ax.set_yticklabels(pair_labels)\n",
    "    ax.set_xticks([0])\n",
    "    ax.set_xticklabels([\"Correlación\"])\n",
    "\n",
    "    # Poner el valor de corr y el nombre del par dentro de la celda (si quieres solo el valor, quita el label)\n",
    "    for i, (label, val) in enumerate(zip(pair_labels, corr_values)):\n",
    "        ax.text(\n",
    "            0, i,\n",
    "            f\"{val:.2f}\",\n",
    "            ha='center', va='center',\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "    plt.colorbar(im, label='Correlación absoluta')\n",
    "    plt.title(f'Top {top_k} pares de variables más correlacionados')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_correlation_with_target(df):\n",
    "    num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "    num_cols = num_cols.drop('target')   # exclude target\n",
    "\n",
    "    # Serie con correlación de cada numérica contra target\n",
    "    corr_with_target = df[num_cols].corrwith(df['target']).dropna()\n",
    "\n",
    "    # Ordenar por valor absoluto (mayor importancia primero)\n",
    "    order = corr_with_target.abs().sort_values(ascending=False).index\n",
    "    corr_sorted = corr_with_target.loc[order]\n",
    "\n",
    "    # Plot horizontal (mayores correlaciones arriba), coloreando por signo\n",
    "    plt.figure(figsize=(6, max(6, len(corr_sorted) * 0.12)))\n",
    "    colors = ['tab:red' if v > 0 else 'tab:blue' for v in corr_sorted.values]\n",
    "    y = np.arange(len(corr_sorted))\n",
    "\n",
    "    plt.barh(y, corr_sorted.values, color=colors)\n",
    "    plt.yticks(y, corr_sorted.index, fontsize=8)\n",
    "    plt.gca().invert_yaxis()  # poner la mayor correlación arriba\n",
    "    plt.xlabel('Correlación con target')\n",
    "    plt.title(\"Correlación de 'target' con variables numéricas (ordenado por |corr|)\")\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    strong_corr = corr_sorted.head(20)\n",
    "\n",
    "    return strong_corr\n",
    "\n",
    "\n",
    "def calculate_lift(df):\n",
    "    num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "    num_cols = num_cols.drop('target')   # exclude target\n",
    "\n",
    "    aux = df[num_cols].groupby(df.target).mean().T\n",
    "    aux['lift'] = aux[1] / aux[0]\n",
    "    aux = aux.sort_values(by='lift', ascending=False)\n",
    "    display(aux.head(20))\n",
    "    display(aux.tail(20))\n",
    "\n",
    "    top_lift = aux.head(20)['lift']\n",
    "    bottom_lift = aux.tail(20)['lift']\n",
    "\n",
    "    strong_lifts = pd.concat([top_lift, bottom_lift])\n",
    "\n",
    "    return strong_lifts\n",
    "\n",
    "def display_nan_percentage(df):\n",
    "    nan_percentage = df.isna().mean() * 100\n",
    "    nan_percentage = nan_percentage[nan_percentage > 0].sort_values(ascending=False).head(20)\n",
    "\n",
    "    display(nan_percentage.head(10).to_frame(name='Top 10 % of NaN values'))\n",
    "    display(nan_percentage.tail(10).to_frame(name='Bottom 10 % of NaN values'))\n",
    "\n",
    "def plot_distributions(df, dtype):\n",
    "\n",
    "    columns = df.select_dtypes(include=dtype).columns\n",
    "\n",
    "    n = len(columns)\n",
    "    ncols = 3\n",
    "    nrows = math.ceil(n / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    if (isinstance(dtype, str) and dtype == 'object') or (isinstance(dtype, (list, tuple)) and 'object' in dtype):\n",
    "        # categorical variables\n",
    "        if len(columns) == 0:\n",
    "            plt.close()\n",
    "            return\n",
    "        for i, col in enumerate(columns):\n",
    "            counts = df[col].value_counts().sort_values(ascending=True).head(20)\n",
    "\n",
    "            axes[i].barh(counts.index.astype(str), counts.values)\n",
    "            axes[i].set_title(col)\n",
    "            axes[i].tick_params(axis='x', rotation=45, pad=3)\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "    else:\n",
    "        for i, col in enumerate(columns):\n",
    "            axes[i].hist(df[col].dropna(), bins=20)\n",
    "            axes[i].axvline(df[col].mean(), color='red', linestyle='--')\n",
    "            axes[i].set_title(col)\n",
    "\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def data_snooping(df):\n",
    "    display(Markdown(f\"The dataset {df.Name} contains {df.shape[0]} observations and {df.shape[1]} columns. \\n\"))\n",
    "\n",
    "    display(Markdown(\"## Number of rows by target class:\"))\n",
    "    display(df.target.value_counts().to_frame())\n",
    "    print()\n",
    "\n",
    "    display(Markdown(\"## Number of columns by data type:\"))\n",
    "    display(df.dtypes.value_counts().to_frame())\n",
    "    print()\n",
    "\n",
    "    display(Markdown(\"## Percentage of NaN values per column:\"))\n",
    "    display_nan_percentage(df)\n",
    "    print()\n",
    "\n",
    "    display(Markdown(\"## Distribution of categorical variables:\"))\n",
    "    plot_distributions(df, dtype='object')\n",
    "    print()\n",
    "\n",
    "    display(Markdown(\"## Distribution of numerical variables:\"))\n",
    "    plot_distributions(df, dtype=['float64','int64'])\n",
    "    print()\n",
    "\n",
    "    display(Markdown(\"## Lift analysis of numerical variables:\"))\n",
    "    strong_lifts = calculate_lift(df)\n",
    "    print()\n",
    "\n",
    "    display(Markdown(\"## Correlation of numerical variables with target:\"))\n",
    "    print()\n",
    "    strong_corr = calculate_correlation_with_target(df)\n",
    "    candidate_features_df = pd.concat([strong_corr.to_frame(name='correlation'), strong_lifts.to_frame(name='lift')], axis=1).sort_values(by='correlation',key=abs, ascending=False)\n",
    "    candidate_features = list(candidate_features_df.index)\n",
    "\n",
    "    display(Markdown(\"## Correlation between candidate features:\"))\n",
    "    calculate_correlation_between_features(df, candidate_features, top_k=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bede515d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_snooping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mdata_snooping\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdata_snooping\u001b[39m(df):\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     display(\u001b[43mMarkdown\u001b[49m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.Name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m observations and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m    165\u001b[39m     display(Markdown(\u001b[33m\"\u001b[39m\u001b[33m## Number of rows by target class:\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    166\u001b[39m     display(df.target.value_counts().to_frame())\n",
      "\u001b[31mNameError\u001b[39m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "data_snooping(app)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
